Step 1 : Import Libraries   --------------------------------------------------------------
import numpy as np
import pandas as pd
from scipy.stats import mode
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
--------------------------------------------------------------------------------------------
Step 2 :   Import Data    ------------------------------------------------------------------
from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv("Path")
--------------------------------------------------------------------------------------------
Step 3 :   Overview    ---------------------------------------------------------------------
df.columns
df.dtypes
df['disease'].values  >>> Target Column
df['disease'].nunique()
df['disease'].value_counts(sort = False)
df.isnull().sum()
--------------------------------------------------------------------------------------------
Step 4 :   Target Column Encoding    -------------------------------------------------------
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df["disease"] = encoder.fit_transform(df["disease"])
--------------------------------------------------------------------------------------------
Step 5 :   Define x And y    ---------------------------------------------------------------
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
---- visualize distribution 
plt.figure(figsize=(10, 5))
sns.countplot(x=y)
plt.title("Disease Class Distribution Before Resampling")
plt.xticks(rotation=90)
plt.show()
--------------------------------------------------------------------------------------------
Step 6 :   Handling Imbalances    ----------------------------------------------------------
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import pandas as pd
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)
print("Resampled Class Distribution:\n", pd.Series(y_resampled).value_counts())
--------------------------------------------------------------------------------------------
Step 7 :   Voting Classifier    ------------------------------------------------------------
### than we will try cross validation method
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Define classifiers
clf_1 = LogisticRegression()
clf_2 = XGBClassifier()
clf_3 = SVC(probability=True)  # Important if you later use soft voting

# Voting classifier
voter = VotingClassifier(estimators=[
    ('lr', clf_1),
    ('xgb', clf_2),
    ('svc', clf_3)
], voting='hard')  # or 'soft' if you prefer probabilities

# Fit individual classifiers
clf_1.fit(X_train, y_train)
clf_2.fit(X_train, y_train)
clf_3.fit(X_train, y_train)

# Fit voting classifier
voter.fit(X_train, y_train)

# Predictions and accuracy
models = {'Logistic Regression': clf_1,
          'XGBoost': clf_2,
          'SVC': clf_3,
          'Voting Classifier': voter}

print("üîç Accuracy Scores:")
for name, model in models.items():
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    train_acc = accuracy_score(y_train, train_pred)
    test_acc = accuracy_score(y_test, test_pred)
    print(f"{name} - Train: {train_acc:.3f}, Test: {test_acc:.3f}")
--------------------------------------------------------------------------------------------
Step 8 :   Cross Validation    -------------------------------------------------------------
clf1 = LogisticRegression()
clf2 = RandomForestClassifier()
clf3 = GradientBoostingClassifier()
# Combine with voting
voting_clf = VotingClassifier(
    estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3)],
    voting='soft'  # soft = use predicted probabilities, hard = majority vote
)

# Cross-validation on the combined model
scores = cross_val_score(voting_clf, X, y, cv=5)
print("VotingClassifier CV scores:", scores)
print("Mean score:", scores.mean())
üí° Key takeaway:
cross_val_score() is only for evaluation ‚Äî it doesn‚Äôt store trained models for later use.
If you want to predict on new data afterward, 
you must train the models again (or load from saved files) before predicting.
--------------------------------------------------------------------------------------------
Step 9 :   train (fit)    ------------------------------------------------------------------
# Final training on full training set
# -----------------------
clf1.fit(X_train, y_train)
clf2.fit(X_train, y_train)
clf3.fit(X_train, y_train)
 -----------------------
# Evaluate on hold-out test set
# -----------------------
test_score1 = clf1.score(X_test, y_test)
test_score2 = clf2.score(X_test, y_test)
test_score3 = clf3.score(X_test, y_test)
print("\nTest set accuracy:")
print(f"Model 1 (LR): {test_score1:.2f}")
print(f"Model 2 (RF): {test_score2:.2f}")
print(f"Model 3 (GB): {test_score3:.2f}")
--------------------------------------------------------------------------------------------
Step 10 :   Prediction Enter New_Data    ----------------------------------------------------
# -----------------------
# Predict for new patient
# -----------------------
new_data = pd.DataFrame([{
    'fever': 1,
    'headache': 0,
    'nausea': 1,
    'vomiting': 0,
    'fatigue': 1,
    'joint_pain': 0,
    'skin_rash': 0,
    'cough': 1,
    'weight_loss': 0,
    'yellow_eyes': 1
}])

pred1 = clf1.predict(new_data)[0]
pred2 = clf2.predict(new_data)[0]
pred3 = clf3.predict(new_data)[0]

print("\nPredictions for new patient:")
print("Model 1 Prediction (LR):", pred1)
print("Model 2 Prediction (RF):", pred2)
print("Model 3 Prediction (GB):", pred3)

# -----------------------
# Majority voting manually
# -----------------------
final_pred = mode([pred1, pred2, pred3], keepdims=False)[0]
print("\nFinal chosen prediction (majority vote):", final_pred)
--------------------------------------------------------------------------------------------
Step 11 :   Grid Search for Each Model    ----------------------------------------------------
We can further fine tune this model to make predictions more accurate.
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# Logistic Regression hyperparameters to tune
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10, 100],        # inverse regularization strength
    'penalty': ['l2'],                    # 'l1' requires solver='liblinear' (can add if needed)
    'solver': ['lbfgs'],
    'max_iter': [1000]
}

# Random Forest hyperparameters to tune
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Gradient Boosting hyperparameters to tune
param_grid_gb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

# Initialize models
clf1 = LogisticRegression(random_state=42)
clf2 = RandomForestClassifier(random_state=42)
clf3 = GradientBoostingClassifier(random_state=42)

# GridSearchCV objects with 5-fold CV
grid_lr = GridSearchCV(clf1, param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1)
grid_rf = GridSearchCV(clf2, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_gb = GridSearchCV(clf3, param_grid_gb, cv=5, scoring='accuracy', n_jobs=-1)

# Fit on training data (replace X_train, y_train with your data)
grid_lr.fit(X_train, y_train)
grid_rf.fit(X_train, y_train)
grid_gb.fit(X_train, y_train)

# Best parameters
print("Best Logistic Regression params:", grid_lr.best_params_)
print("Best Random Forest params:", grid_rf.best_params_)
print("Best Gradient Boosting params:", grid_gb.best_params_)

# Best scores
print("Best Logistic Regression CV accuracy:", grid_lr.best_score_)
print("Best Random Forest CV accuracy:", grid_rf.best_score_)
print("Best Gradient Boosting CV accuracy:", grid_gb.best_score_)
## Best Estimators
best_lr = grid_lr.best_estimator_
best_rf = grid_rf.best_estimator_
best_gb = grid_gb.best_estimator_

# Example prediction on new data
new_data = pd.DataFrame([{
    'fever': 1,
    'headache': 0,
    'nausea': 1,
    'vomiting': 0,
    'fatigue': 1,
    'joint_pain': 0,
    'skin_rash': 0,
    'cough': 1,
    'weight_loss': 0,
    'yellow_eyes': 1
}])

print("Predicted by best Logistic Regression:", best_lr.predict(new_data)[0])
print("Predicted by best Random Forest:", best_rf.predict(new_data)[0])
print("Predicted by best Gradient Boosting:", best_gb.predict(new_data)[0])



















