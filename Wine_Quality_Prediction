In these 4 model  we have four different scenarios from Simple to Advanced
1) This is Simple Prediction Model Without advanced Steps.
2) Model With Cross Validation.
3) Model With Scaling (MinMax And Standard)
4) Voting Classifiers 
------------------------------------------------------------------------------------
1) **This is Simple Prediction Model Without advanced Steps**
------------------------------------------------------------------------------------
----------------Connect to Google Drive-----------------------------------------
from google.colab import drive
drive.mount('/content/drive')
-----------------------import Data----------------------------------------------
import pandas as pd
df = pd.read_csv("winequalityN.csv")
df.head()
----------------Check Data Type And NULL's---------------------------------------
df.columns
df.dtypes
df.isnull().sum()
df = df.dropna()  
--or  we have other option instead of drop null rows fill na
df['ColumnName'].fillna(df['ColumnName'].median(), inplace=True)
----------------------Simple Encoding-------------------------------------------
df['quality'] = [1 if x > 5 else 0 for x in df.quality]
df['type'] = [1 if x == "white" else 0 for x in df.type]
--------------------Modeling And Accuracy-----------------------------------------
X = df.drop('quality', axis=1)
y = df['quality']
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=42)
model=LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
Accuracy Score: 72.24%
------------------------------------------------------------------------------------
2) **With Cross Validation**
------------------------------------------------------------------------------------
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
model=LogisticRegression()
scores = cross_val_score(model1, X, y, cv=5, scoring='accuracy')
print("Cross-Validation Scores:", scores)
print("Mean Accuracy: {:.2f}%".format(np.mean(scores) * 100))
Cross-Validation Scores: [0.64114462 0.70843001 0.72699149 0.76393189 0.46362229]
Mean Accuracy: 66.08%
------------------------------------------------------------------------------------
3) **With Scaling**
------------------------------------------------------------------------------------
----------------Connect to Google Drive-----------------------------------------
from google.colab import drive
drive.mount('/content/drive')
-----------------------import Data----------------------------------------------
import pandas as pd
df = pd.read_csv("winequalityN.csv")
df.head()
----------------Check Data Type And NULL's---------------------------------------
df.columns
df.dtypes
df.isnull().sum()
df = df.dropna()  
--or  we have other option instead of drop null rows fill na
df['ColumnName'].fillna(df['ColumnName'].median(), inplace=True)
df.describe()
----------------------Simple Encoding-------------------------------------------
df['quality'] = [1 if x > 5 else 0 for x in df.quality]
df['type'] = [1 if x == "white" else 0 for x in df.type]
--------------------Modeling And Accuracy-----------------------------------------
X = df.drop('quality', axis=1)
y = df['quality']
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=42)
scaler = MinMaxScaler() 
Or Another method 
scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
model=LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
Accuracy: 75.10%
------------------------------------------------------------------------------------
4) **Ensamble-Voting**
------------------------------------------------------------------------------------
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Define classifiers
clf_1 = LogisticRegression()
clf_2 = XGBClassifier()
clf_3 = SVC(probability=True)  # Important if you later use soft voting

# Voting classifier
voter = VotingClassifier(estimators=[
    ('lr', clf_1),
    ('xgb', clf_2),
    ('svc', clf_3)
], voting='hard')  # or 'soft' if you prefer probabilities

# Fit individual classifiers
clf_1.fit(X_train, y_train)
clf_2.fit(X_train, y_train)
clf_3.fit(X_train, y_train)

# Fit voting classifier
voter.fit(X_train, y_train)

# Predictions and accuracy
models = {'Logistic Regression': clf_1,
          'XGBoost': clf_2,
          'SVC': clf_3,
          'Voting Classifier': voter}

print("ğŸ” Accuracy Scores:")
for name, model in models.items():
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    train_acc = accuracy_score(y_train, train_pred)
    test_acc = accuracy_score(y_test, test_pred)
    print(f"{name} - Train: {train_acc:.3f}, Test: {test_acc:.3f}")

ğŸ” Accuracy Scores:
Logistic Regression - Train: 0.746, Test: 0.751
XGBoost - Train: 0.985, Test: 0.803
SVC - Train: 0.761, Test: 0.759
Voting Classifier - Train: 0.787, Test: 0.769
------------------------------------------------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler

# áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ
df = pd.read_csv(r"D:\2. Personal\DataScience\winequalityN.csv")

# áƒ’áƒáƒ£áƒœáƒ£áƒšáƒ”áƒ‘áƒ”áƒšáƒ˜ áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ¬áƒáƒ¨áƒšáƒ
df = df.dropna()

# áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒ£áƒšáƒ˜ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ¥áƒ£áƒšáƒáƒ“ áƒ’áƒáƒ“áƒáƒ§áƒ•áƒáƒœáƒ
df['quality'] = [1 if x > 5 else 0 for x in df.quality]
df['type'] = [1 if x == "white" else 0 for x in df.type]

# áƒªáƒ•áƒšáƒáƒ“áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ
X = df.drop('quality', axis=1)
y = df['quality']

# áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ’áƒáƒ§áƒáƒ¤áƒ áƒ¡áƒáƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒáƒ“ áƒ“áƒ áƒ¡áƒáƒ¢áƒ”áƒ¡áƒ¢áƒáƒ“
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# áƒ›áƒáƒ¡áƒ¨áƒ¢áƒáƒ‘áƒ˜áƒ áƒ”áƒ‘áƒ MinMaxScaler-áƒ˜áƒ—
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ áƒ“áƒ áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜
model = LogisticRegression()
model.fit(X_train, y_train)

# áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜áƒ áƒ”áƒ‘áƒ
y_pred = model.predict(X_test)

# áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ”áƒ‘áƒ˜áƒ¡ DataFrame-áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ
X_test_df = pd.DataFrame(X_test, columns=X.columns, index=y_test.index)
result_df = X_test_df.copy()
result_df['y_pred'] = y_pred
result_df['y_true'] = y_test

# áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ
print(result_df.head())

# áƒ¡áƒ£áƒ áƒ•áƒ˜áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ áƒ¨áƒ”áƒ˜áƒœáƒáƒ®áƒ” Excel-áƒ¨áƒ˜
# result_df.to_excel("wine_predictions.xlsx", index=False)














