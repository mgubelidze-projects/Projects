------------------------------------------------------------------------------------
**This is Simple Prediction Model Without advanced Steps**
------------------------------------------------------------------------------------
----------------Connect to Google Drive-----------------------------------------
from google.colab import drive
drive.mount('/content/drive')
-----------------------import Data----------------------------------------------
import pandas as pd
df = pd.read_csv("winequalityN.csv")
df.head()
----------------Check Data Type And NULL's---------------------------------------
df.columns
df.dtypes
df.isnull().sum()
df = df.dropna()  
--or  we have other option instead of drop null rows fill na
df['ColumnName'].fillna(df['ColumnName'].median(), inplace=True)
----------------------Simple Encoding-------------------------------------------
df['quality'] = [1 if x > 5 else 0 for x in df.quality]
df['type'] = [1 if x == "white" else 0 for x in df.type]
--------------------Modeling And Accuracy-----------------------------------------
X = df.drop('quality', axis=1)
y = df['quality']
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=42)
model=LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
Accuracy Score: 72.24%
------------------------------------------------------------------------------------
**With Cross Validation**
------------------------------------------------------------------------------------
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
model=LogisticRegression()
scores = cross_val_score(model1, X, y, cv=5, scoring='accuracy')
print("Cross-Validation Scores:", scores)
print("Mean Accuracy: {:.2f}%".format(np.mean(scores) * 100))
Cross-Validation Scores: [0.64114462 0.70843001 0.72699149 0.76393189 0.46362229]
Mean Accuracy: 66.08%
------------------------------------------------------------------------------------
**With Scaling**
------------------------------------------------------------------------------------
----------------Connect to Google Drive-----------------------------------------
from google.colab import drive
drive.mount('/content/drive')
-----------------------import Data----------------------------------------------
import pandas as pd
df = pd.read_csv("winequalityN.csv")
df.head()
----------------Check Data Type And NULL's---------------------------------------
df.columns
df.dtypes
df.isnull().sum()
df = df.dropna()  
--or  we have other option instead of drop null rows fill na
df['ColumnName'].fillna(df['ColumnName'].median(), inplace=True)
df.describe()
----------------------Simple Encoding-------------------------------------------
df['quality'] = [1 if x > 5 else 0 for x in df.quality]
df['type'] = [1 if x == "white" else 0 for x in df.type]
--------------------Modeling And Accuracy-----------------------------------------
X = df.drop('quality', axis=1)
y = df['quality']
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=42)
scaler = MinMaxScaler() 
Or Another method 
scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
model=LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
Accuracy: 75.10%
------------------------------------------------------------------------------------
**Ensamble-Voting**
------------------------------------------------------------------------------------
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Define classifiers
clf_1 = LogisticRegression()
clf_2 = XGBClassifier()
clf_3 = SVC(probability=True)  # Important if you later use soft voting

# Voting classifier
voter = VotingClassifier(estimators=[
    ('lr', clf_1),
    ('xgb', clf_2),
    ('svc', clf_3)
], voting='hard')  # or 'soft' if you prefer probabilities

# Fit individual classifiers
clf_1.fit(X_train, y_train)
clf_2.fit(X_train, y_train)
clf_3.fit(X_train, y_train)

# Fit voting classifier
voter.fit(X_train, y_train)

# Predictions and accuracy
models = {'Logistic Regression': clf_1,
          'XGBoost': clf_2,
          'SVC': clf_3,
          'Voting Classifier': voter}

print("üîç Accuracy Scores:")
for name, model in models.items():
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    train_acc = accuracy_score(y_train, train_pred)
    test_acc = accuracy_score(y_test, test_pred)
    print(f"{name} - Train: {train_acc:.3f}, Test: {test_acc:.3f}")

üîç Accuracy Scores:
Logistic Regression - Train: 0.746, Test: 0.751
XGBoost - Train: 0.985, Test: 0.803
SVC - Train: 0.761, Test: 0.759
Voting Classifier - Train: 0.787, Test: 0.769
------------------------------------------------------------------------------------------------------------------------------------------------------------------













