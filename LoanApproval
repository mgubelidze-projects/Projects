import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#---------------------------------------#
from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv("path")
#---------------------------------------#
  Data Preprocessing and Visualization
#---------------------------------------#
# count number of object type columns
obj = (df.dtypes == 'object')
print("Categorical variables:",len(list(obj[obj].index)))
#---------------------------------------#
# Dropping Loan_ID column
df.drop(['Loan_ID'],axis=1,inplace=True)
#---------------------------------------#
Visualize all the unique values in columns using barplot. 
This will simply show which value is dominating as per our dataset.

obj = (df.dtypes == 'object')
object_cols = list(obj[obj].index)
plt.figure(figsize=(18,36))
index = 1

for col in object_cols:
  y = df[col].value_counts()
  plt.subplot(11,4,index)
  plt.xticks(rotation=90)
  sns.barplot(x=list(y.index), y=y)
  index +=1 
#-----------label encoder-------------#
from sklearn import preprocessing 
# label_encoder object knows how 
# to understand word labels.
label_encoder = preprocessing.LabelEncoder()
obj = (df.dtypes == 'object')
for col in list(obj[obj].index):
  df[col] = label_encoder.fit_transform(df[col])
#------Correlation Using HeatMap-------#
plt.figure(figsize=(12,6))
sns.heatmap(data.corr(),cmap='BrBG',fmt='.2f',
            linewidths=2,annot=True)
#------Check Missing values-------#
for col in df.columns:
  df[col] = df[col].fillna(df[col].mean())   
df.isna().sum()
#------splitting DataSet-----------#
from sklearn.model_selection import train_test_split

X = df.drop(['Loan_Status'],axis=1)
Y = df['Loan_Status']
X.shape,Y.shape
X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                    test_size=0.4,
                                                    random_state=1)
X_train.shape, X_test.shape, Y_train.shape, Y_test.shape

#------Model Training and Evaluation-----------#
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

from sklearn import metrics

knn = KNeighborsClassifier(n_neighbors=3)
rfc = RandomForestClassifier(n_estimators = 7,
                             criterion = 'entropy',
                             random_state =7)
svc = SVC()
lc = LogisticRegression()

# making predictions on the training set
for clf in (rfc, knn, svc,lc):
    clf.fit(X_train, Y_train)
    Y_pred = clf.predict(X_train)
    print("Accuracy score of ",
          clf.__class__.__name__,
          "=",100*metrics.accuracy_score(Y_train, 
                                         Y_pred))
#------Making predictions on the testing set-----------#
for clf in (rfc, knn, svc,lc):
    clf.fit(X_train, Y_train)
    Y_pred = clf.predict(X_test)
    print("Accuracy score of ",
          clf.__class__.__name__,"=",
          100*metrics.accuracy_score(Y_test,
                                     Y_pred))

#------ Predict for New Data --------#
new_data = {
    'Gender': 'Female',
    'Married': 'No',
    'Dependents': '0',
    'Education': 'Graduate',
    'Self_Employed': 'No',
    'ApplicantIncome': 1000,
    'CoapplicantIncome': 0,
    'LoanAmount': 150,
    'Loan_Amount_Term': 360,
    'Credit_History': 1.0,
    'Property_Area': 'Urban'
}

new_df = pd.DataFrame([new_data])

# Ensure same column order as training data
new_df = new_df[X_train.columns]

from sklearn import preprocessing 
# label_encoder object knows how 
# to understand word labels.
label_encoder = preprocessing.LabelEncoder()
obj = (new_df.dtypes == 'object')
for col in list(obj[obj].index):
  new_df[col] = label_encoder.fit_transform(new_df[col])

# Predict
prediction = best_model.predict(new_df)[0]
status_map = {1: "Approved", 0: "Rejected"}
print("\nPrediction for new data:", status_map.get(prediction, prediction))











